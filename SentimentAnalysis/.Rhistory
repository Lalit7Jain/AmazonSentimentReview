reviews.sent <- dfm(myCorpus, stem = TRUE, removePunct = TRUE,dictionary = myDict,toLower = TRUE,removeNumbers = TRUE)
reviews.sent
## Making a dataframe
sentiment.df <- as.data.frame(as.matrix(reviews.sent))
sentiment.df$document <- rownames(sentiment.df)
## Removing reviews which does not have positive or negative count
sentiment.df <- sentiment.df[which(sentiment.df$posit != 0 | sentiment.df$negat != 0),]
write.csv(sentiment.df,"reviews/ReviewSentiment.csv")
## Visualizing individual reviews sentiment
library(reshape2)
dat_l <- melt(sentiment.df, id.vars = c("document"))
library(ggplot2)
p <- ggplot(data = dat_l, aes(x = document, y = value, group = variable, fill = variable))
p <- p + geom_bar(stat = "identity", width = 0.5, position = "dodge")
p <- p + theme_bw()
p <- p + theme(axis.text.x = element_text(angle = 90))
png(filename="reviews/ReviewSentiment.png")
plot(p)
dev.off()
## computing and visualizing overall sentiment
overall.sentiment <- data.frame(Positive = sum(sentiment.df$posit)/nrow(sentiment.df),
Negative = sum(sentiment.df$negat/nrow(sentiment.df)),
Review = "Overall Review")
write.csv(overall.sentiment,"reviews/OverallSentiment.csv")
dat_l2 <- melt(overall.sentiment, id.vars = c("Review"))
p2 <- ggplot(data = dat_l2, aes(x = Review, y = value, group = variable, fill = variable))
p2 <- p2 + geom_bar(stat = "identity", width = 0.5, position = "dodge")
p2 <- p2 + theme_bw()
png(filename="reviews/OverallSentiment.png")
plot(p2)
dev.off()
View(sentiment.df)
View(overall.sentiment)
View(dat_l)
texts(myCorpus)[1]
View(sentiment.df)
View(sentiment.df)
sentiment.df$Review <- NULL
View(sentiment.df)
sentiment.df <- as.data.frame(as.matrix(reviews.sent))
sentiment.df$document <- rownames(sentiment.df)
View(sentiment.df)
sentiment.df$Review <- NULL
length(myCorpus)
length(texts(myCorpus))
for(i in 1:length(texts(myCorpus))){
sentiment.df[i,"Review"] <- texts(myCorpus)[i]
}
View(sentiment.df)
sentiment.df <- sentiment.df[which(sentiment.df$posit != 0 | sentiment.df$negat != 0),]
View(sentiment.df)
View(sentiment.df)
library(dplyr)
View(sentiment.df)
sentiment.final <- sentiment.df %>% mutate(label = if_else(posit>negat,"Positive","Negative"))
View(sentiment.final)
str(sentimend.d )
str(sentiment.df)
str(sentiment.final)
library(reshape2)
dat_l <- melt(sentiment.df, id.vars = c("document"))
library(ggplot2)
p <- ggplot(data = dat_l, aes(x = document, y = value, group = variable, fill = variable))
p <- p + geom_bar(stat = "identity", width = 0.5, position = "dodge")
p <- p + theme_bw()
p <- p + theme(axis.text.x = element_text(angle = 90))
plot(p)
sentiment.df <- sentiment.df %>% mutate(label = if_else(posit>negat,"Positive","Negative"))
library(reshape2)
dat_l <- melt(sentiment.df, id.vars = c("document"))
View(dat_l)
library(ggplot2)
p <- ggplot(data = dat_l, aes(x = document, y = value, group = variable, fill = variable))
p <- p + geom_bar(stat = "identity", width = 0.5, position = "dodge")
p <- p + theme_bw()
p <- p + theme(axis.text.x = element_text(angle = 90))
p
overall.sentiment <- data.frame(Positive = sum(sentiment.df$posit)/nrow(sentiment.df),
Negative = sum(sentiment.df$negat/nrow(sentiment.df)),
Review = "Overall Review")
dat_l2 <- melt(overall.sentiment, id.vars = c("Review"))
p2 <- ggplot(data = dat_l2, aes(x = Review, y = value, group = variable, fill = variable))
p2 <- p2 + geom_bar(stat = "identity", width = 0.5, position = "dodge")
p2 <- p2 + theme_bw()
plot(p2)
plot(p)
View(dat_l)
View(dat_l)
View(sentiment.df)
dat_l <- melt(sentiment.df[,1:3], id.vars = c("document"))
View(dat_l)
library(ggplot2)
p <- ggplot(data = dat_l, aes(x = document, y = value, group = variable, fill = variable))
p <- p + geom_bar(stat = "identity", width = 0.5, position = "dodge")
p <- p + theme_bw()
p <- p + theme(axis.text.x = element_text(angle = 90))
plot(p)
View(sentiment.df)
page = 5
p = 1
for (i in 1:page){
newpage <- paste(review.page,"&pageNumber=",i,sep = "")
readreviewpage <- read_html(x=newpage)
reviews <- readreviewpage %>% html_nodes(xpath = XPATH_REVIEW_SECTION_2) %>%
html_nodes(xpath = XPATH_REVIEW_SECTION_3) %>% html_text()
if(length(reviews) > 0){
for (j in 1:length(reviews)){
r <- as.character(reviews[j])
filename <- paste("reviews/Review",p,".txt",sep = "")
write(r,file = filename)
p = p+1
}
}
}
library(dplyr)
library(rvest)
page = 5
p = 1
for (i in 1:page){
newpage <- paste(review.page,"&pageNumber=",i,sep = "")
readreviewpage <- read_html(x=newpage)
reviews <- readreviewpage %>% html_nodes(xpath = XPATH_REVIEW_SECTION_2) %>%
html_nodes(xpath = XPATH_REVIEW_SECTION_3) %>% html_text()
if(length(reviews) > 0){
for (j in 1:length(reviews)){
r <- as.character(reviews[j])
filename <- paste("reviews/Review",p,".txt",sep = "")
write(r,file = filename)
p = p+1
}
}
}
library("tm")
library("SnowballC")
require(quanteda)
reviewsInput <- quanteda::textfile("reviews/*.txt")
str(reviewsInput)
myCorpus <- corpus(reviewsInput)
texts(myCorpus)[1]
myDict <- dictionary(list(positive = pos.words,negative = neg.words))
reviews.sent <- dfm(myCorpus, stem = TRUE, removePunct = TRUE,dictionary = myDict,toLower = TRUE,removeNumbers = TRUE)
reviews.sent
sentiment.df <- as.data.frame(as.matrix(reviews.sent))
sentiment.df$document <- rownames(sentiment.df)
## Adding all the reviews
sentiment.df$Review <- NULL
for(i in 1:length(texts(myCorpus))){
sentiment.df[i,"Review"] <- texts(myCorpus)[i]
}
sentiment.df <- sentiment.df[which(sentiment.df$posit != 0 | sentiment.df$negat != 0),]
View(sentiment.df)
library(dplyr)
sentiment.df <- sentiment.df %>% mutate(label = if_else(posit>negat,"Positive","Negative"))
View(sentiment.df)
View(sentiment.df)
sentiment.df <- select(sentiment.df,(Review,label))
?select
sentiment.df <- select(sentiment.df,Review,label)
View(sentiment.df)
write.csv(sentiment.df,"reviews/ReviewSentiment.csv")
reviews.sent
reviews.sent[1]
reviews.sent$docs[1]
methods(reviews.sent)
reviews.sent[,1:5]
reviews.sent[,1:2]
reviews.sent[,1:3]
reviews.sent[1,1]
reviews.sent[1"5",1]
reviews.sent[1:5,1]
reviews.sent[1:5,1:2]
topfeatures(reviews.sent, 20)  # 20 top words
set.seed(20)
textplot_wordcloud(dfm_trim(reviews.sent, min_count = 6))
require(quanteda)
textplot_wordcloud(dfm_trim(reviews.sent, min_count = 6))
?tidy
install.packages("tidytext")
require(tidytext)
tidy(reviews.sent)
library(dplyr)
library(tidytext)
amazonreview_td <- tidy(reviewsInput)
amazonreview_td <- tidy(myCorpus)
View(amazonreview_td)
myCorpus
reviews.dfm <- dfm(myCorpus, stem = TRUE, removePunct = TRUE,toLower = TRUE,removeNumbers = TRUE)
amazonreview_td <- tidy(myCorpus)
View(amazonreview_td)
reviews.dfm
myCorpus$documents
myCorpus <- Corpus(VectorSource(myCorpus$documents)) #converts the relevant part of your file into a corpus
myCorpus = tm_map(myCorpus, PlainTextDocument) # an intermediate preprocessing step
myCorpus = tm_map(myCorpus, tolower) # converts all text to lower case
myCorpus = tm-map(myCorpus, removePunctuation) #removes punctuation
myCorpus = tm_map(myCorpus, removePunctuation) #removes punctuation
myCorpus = tm_map(myCorpus, removeWords, stopwords("english")) #removes common words like "a", "the" etc
myCorpus = tm_map(myCorpus, stemDocument) # removes the last few letters of similar words such as get, getting, gets
dtm = DocumentTermMatrix(myCorpus) #turns the corpus into a document term matrix
dtm
amazonreview_td <- tidy(dtm)
View(amazonreview_td)
amazonreview_sentiments <- amazonreview_td %>%
inner_join(get_sentiments("bing"), by = c(term = "word"))
amazonreview_sentiments
library(tidyr)
amazonreview_sentiments %>%
count(document, sentiment, wt = count) %>%
ungroup() %>%
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative) %>%
arrange(sentiment)
library(ggplot2)
amazonreview_sentiments %>%
count(sentiment, term, wt = count) %>%
ungroup() %>%
filter(n >= 150) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(term = reorder(term, n)) %>%
ggplot(aes(term, n, fill = sentiment)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ylab("Contribution to sentiment")
amazonreview_sentiments
amazonreview_sentiments %>%
count(sentiment, term, wt = count) %>%
ungroup() %>%
filter(n >= 10) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(term = reorder(term, n)) %>%
ggplot(aes(term, n, fill = sentiment)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ylab("Contribution to sentiment")
amazonreview_sentiments %>%
count(sentiment, term, wt = count) %>%
ungroup() %>%
filter(n >= 2) %>%
mutate(n = ifelse(sentiment == "negative", -n, n)) %>%
mutate(term = reorder(term, n)) %>%
ggplot(aes(term, n, fill = sentiment)) +
geom_bar(stat = "identity") +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) +
ylab("Contribution to sentiment")
reviewsInput <- quanteda::textfile("reviews/*.txt")
str(reviewsInput)
myCorpus <- corpus(reviewsInput)
texts(myCorpus)[1]
reviews.sent <- dfm(myCorpus, stem = TRUE, removePunct = TRUE,dictionary = myDict,toLower = TRUE,removeNumbers = TRUE)
reviews.sent
summary(reviews.sent)
reviews.sent
reviews.sent[1:2,1:2]
reviews.sent <- dfm(myCorpus)
reviews.sent[,1:5]
reviews.sent
reviews.sent[1,1:10]
head(reviews.sent)
docnames(reviews.sent[1,])
library(dplyr)
library(tidytext)
myCorpus <- Corpus(VectorSource(myCorpus$documents)) #converts the relevant part of your file into a corpus
myCorpus = tm_map(myCorpus, PlainTextDocument) # an intermediate preprocessing step
myCorpus = tm_map(myCorpus, tolower) # converts all text to lower case
myCorpus = tm_map(myCorpus, removePunctuation) #removes punctuation
myCorpus = tm_map(myCorpus, removeWords, stopwords("english")) #removes common words like "a", "the" etc
myCorpus = tm_map(myCorpus, stemDocument)
myCorpus
myCorpus$`1`
reviewsInput <- quanteda::textfile("reviews/*.txt")
str(reviewsInput)
myCorpus <- corpus(reviewsInput)
myCorpus <- Corpus(VectorSource(myCorpus$documents)) #converts the relevant part of your file into a corpus
myCorpus = tm_map(myCorpus, PlainTextDocument) # an intermediate preprocessing step
myCorpus = tm_map(myCorpus, tolower) # converts all text to lower case
myCorpus = tm_map(myCorpus, removePunctuation) #removes punctuation
myCorpus = tm_map(myCorpus, removeWords, stopwords("english")) #removes common words like "a", "the" etc
myCorpus = tm_map(myCorpus, stemDocument)
myCorpus$`1`
reviewsInput <- quanteda::textfile("reviews/*.txt")
str(reviewsInput)
myCorpus <- corpus(reviewsInput)
myCorpus$documents
myCorpus <- Corpus(VectorSource(myCorpus$documents)) #converts the relevant part of your file into a corpus
reviewsInput <- quanteda::textfile("reviews/*.txt")
str(reviewsInput)
myCorpus <- corpus(reviewsInput)
myCorpus <- Corpus(VectorSource(myCorpus)) #converts the relevant part of your file into a corpus
myCorpus$`1`
myCorpus$`2`
myCorpus
reviewsInput <- quanteda::textfile("reviews/*.txt")
str(reviewsInput)
myCorpus <- corpus(reviewsInput)
myCorpus = tm_map(myCorpus, PlainTextDocument) # an intermediate preprocessing step
myCorpus <- Corpus(VectorSource(myCorpus)) #converts the relevant part of your file into a corpus
myCorpus = tm_map(myCorpus, PlainTextDocument) # an intermediate preprocessing step
myCorpus = tm_map(myCorpus, tolower) # converts all text to lower case
myCorpus = tm_map(myCorpus, removePunctuation) #removes punctuation
myCorpus = tm_map(myCorpus, removeWords, stopwords("english")) #removes common words like "a", "the" etc
myCorpus = tm_map(myCorpus, stemDocument) # removes the last few letters of similar words such as get, getting, gets
myCorpus
myCorpusp1
myCorpus[1]
head(myCorpus)
dtm = DocumentTermMatrix(myCorpus) #turns the corpus into a document term matrix
dtm
head(dtm)
summary(dtm)
dtm
amazonreview_td <- tidy(dtm)
amazonreview_td
myCorpus = tm_map(myCorpus, removeNumbers)
dtm = DocumentTermMatrix(myCorpus) #turns the corpus into a document term matrix
amazonreview_td <- tidy(dtm)
amazonreview_td
View(sentiment.df)
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")
reviews.read <- read.csv("reviews/ReviewSentiment.csv")
View(reviews.read)
View(reviews.read)
View(reviews.read)
reviews.read <- read.csv("reviews/ReviewSentiment.csv")  %>%
# converting some symbols
dmap_at('Review', conv_fun) %>%
# replacing class values
mutate(sentiment = ifelse(label == "Positive", 1, 0))
library(ROAuth)
install.packages("tidyverse")
library(tidyverse)
conv_fun <- function(x) iconv(x, "latin1", "ASCII", "")
reviews.read <- read.csv("reviews/ReviewSentiment.csv")  %>%
# converting some symbols
dmap_at('Review', conv_fun) %>%
# replacing class values
mutate(sentiment = ifelse(label == "Positive", 1, 0))
library(glmnet)
library(ggrepel)
install.packages("ggrepel")
library(ggrepel)
reviews.read <- read.csv("reviews/ReviewSentiment.csv")  %>%
# converting some symbols
dmap_at('Review', conv_fun) %>%
# replacing class values
mutate(sentiment = ifelse(label == "Positive", 1, 0))
install.packages("ROAuth")
library(ROAuth)
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
reviews.read <- read.csv("reviews/ReviewSentiment.csv")  %>%
# converting some symbols
dmap_at('Review', conv_fun) %>%
# replacing class values
mutate(sentiment = ifelse(label == "Positive", 1, 0))
View(reviews.read)
View(reviews.read)
reviews.read <- read.csv("reviews/ReviewSentiment.csv")  %>%
# converting some symbols
dmap_at('Review', conv_fun) %>%
# replacing class values
mutate(sentiment = ifelse(label == "Positive", 1, 0)) %>% select(Review,sentiment)
View(reviews.read)
set.seed(232)
trainIndex <- createDataPartition(reviews.read$sentiment, p = 0.8,
list = FALSE,
times = 1)
amazon_train <- tweets_classified[trainIndex, ]
amazon_test <- tweets_classified[-trainIndex, ]
library(caret)
set.seed(232)
trainIndex <- createDataPartition(reviews.read$sentiment, p = 0.8,
list = FALSE,
times = 1)
amazon_train <- reviews.read[trainIndex, ]
amazon_test <- reviews.read[-trainIndex, ]
View(amazon_train)
prep_fun <- tolower
tok_fun <- word_tokenizer
library(text2vec)
install.packages("text2vec")
prep_fun <- tolower
tok_fun <- word_tokenizer
View(prep_fun)
library(tidyverse)
library(text2vec)
tok_fun <- word_tokenizer
View(tok_fun)
it_train <- itoken(amazon_train$text,
preprocessor = prep_fun,
tokenizer = tok_fun,
#ids = amazon_train$id,
progressbar = TRUE)
it_train <- itoken(amazon_train$Review,
preprocessor = prep_fun,
tokenizer = tok_fun,
#ids = amazon_train$id,
progressbar = TRUE)
it_train
?itoken
it_train$iterable
vocab <- create_vocabulary(it_train)
vectorizer <- vocab_vectorizer(vocab)
dtm_train <- create_dtm(it_train, vectorizer)
vocab
vectorizer
vectorizer <- vocab_vectorizer(vocab)
?vocab_vectorizer
dtm_train <- create_dtm(it_train, vectorizer)
dtm_test <- create_dtm(it_test, vectorizer)
it_test <- itoken(amazon_test$Review,
preprocessor = prep_fun,
tokenizer = tok_fun,
progressbar = TRUE)
dtm_test <- create_dtm(it_test, vectorizer)
tfidf <- TfIdf$new()
dtm_train_tfidf <- fit_transform(dtm_train, tfidf)
dtm_test_tfidf <- fit_transform(dtm_test, tfidf)
amazon_train[['sentiment']]
nrow(dtm_train_tfidf)
head(dtm_train_tfidf)
dtm_train_tfidf
# train the model
t1 <- Sys.time()
glmnet_classifier <- cv.glmnet(x = dtm_train_tfidf, y = amazon_train[['sentiment']],
family = 'binomial',
# L1 penalty
alpha = 1,
# interested in the area under ROC curve
type.measure = "auc",
# 5-fold cross-validation
nfolds = 5,
# high value is less accurate, but has faster training
thresh = 1e-3,
# again lower number of iterations for faster training
maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'mins'))
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
preds <- predict(glmnet_classifier, dtm_test_tfidf, type = 'response')[ ,1]
glmnet:::auc(as.numeric(amazon_test$sentiment), preds)
preds
as.numeric(amazon_test$sentiment)
?auc
glmnet:::auc(as.numeric(amazon_test$sentiment), preds)
glmnet:::auc(as.numeric(amazon_test$sentiment), preds)
set.seed(232)
trainIndex <- createDataPartition(reviews.read$sentiment, p = 0.8,
list = FALSE,
times = 1)
amazon_train <- reviews.read[trainIndex, ]
amazon_test <- reviews.read[-trainIndex, ]
View(amazon_test)
trainIndex
library("tm")
library("SnowballC")
pos <- scan('positive-words.txt', what='character', comment.char=';')
neg <- scan('negative-words.txt', what='character', comment.char=';')
pos.words <- c(pos, 'upgrade')
neg.words <- c(neg, 'wtf', 'wait', 'waiting', 'epicfail')
require(quanteda)
reviewsInput <- quanteda::textfile("reviews/*.txt")
str(reviewsInput)
myCorpus <- corpus(reviewsInput)
## making a dictionary of positive and negative words
myDict <- dictionary(list(positive = pos.words,negative = neg.words))
## Quanteda will create Document Frequency Matrix by function dfm(). This function
## essentially does this by series of operation including tokenizing, lowercasing, indexing
reviews.sent <- dfm(myCorpus, stem = TRUE, removePunct = TRUE,dictionary = myDict,toLower = TRUE,removeNumbers = TRUE)
reviews.sent
###
require(tidytext)
tidy(reviews.sent)
## Making a dataframe
sentiment.df <- as.data.frame(as.matrix(reviews.sent))
sentiment.df$document <- rownames(sentiment.df)
View(sentiment.df)
## Adding all the reviews
sentiment.df$Review <- NULL
for(i in 1:length(texts(myCorpus))){
sentiment.df[i,"Review"] <- texts(myCorpus)[i]
}
View(sentiment.df)
## Removing reviews which does not have positive or negative count
sentiment.df <- sentiment.df[which(sentiment.df$posit != 0 | sentiment.df$negat != 0),]
## labelling the document
library(dplyr)
sentiment.df <- sentiment.df %>% mutate(label = if_else(posit>negat,"Positive","Negative"))
library(reshape2)
dat_l <- melt(sentiment.df[1:20,1:3], id.vars = c("document"))
library(ggplot2)
p <- ggplot(data = dat_l, aes(x = document, y = value, group = variable, fill = variable))
p <- p + geom_bar(stat = "identity", width = 0.5, position = "dodge")
p <- p + theme_bw()
p <- p + theme(axis.text.x = element_text(angle = 90))
plot(p)
overall.sentiment <- data.frame(Positive = sum(sentiment.df$posit)/nrow(sentiment.df),
Negative = sum(sentiment.df$negat/nrow(sentiment.df)),
Review = "Overall Review")
write.csv(overall.sentiment,"reviews/OverallSentiment.csv")
dat_l2 <- melt(overall.sentiment, id.vars = c("Review"))
p2 <- ggplot(data = dat_l2, aes(x = Review, y = value, group = variable, fill = variable))
p2 <- p2 + geom_bar(stat = "identity", width = 0.5, position = "dodge")
p2 <- p2 + theme_bw()
plot(p2)
dim(sentiment.df)
sentiment.df_2 <- select(sentiment.df[1:1000,],Review,label)
write.csv(sentiment.df,"reviews/ReviewSentiment_2.csv")
dim(sentiment.df[1:1000,])
write.csv(sentiment.df_2,"reviews/ReviewSentiment_2.csv")
sentiment.df_2 <- select(sentiment.df[1:3000,],Review,label)
write.csv(sentiment.df_2,"reviews/ReviewSentiment_2.csv")
