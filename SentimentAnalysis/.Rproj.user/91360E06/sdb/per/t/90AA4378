{
    "collab_server" : "",
    "contents" : "library(dplyr)\nlibrary(tidytext)\n\nmyCorpus <- Corpus(VectorSource(myCorpus)) #converts the relevant part of your file into a corpus\n\nmyCorpus = tm_map(myCorpus, PlainTextDocument) # an intermediate preprocessing step\n\nmyCorpus = tm_map(myCorpus, tolower) # converts all text to lower case\n\nmyCorpus = tm_map(myCorpus, removePunctuation) #removes punctuation\n\nmyCorpus = tm_map(myCorpus, removeWords, stopwords(\"english\")) #removes common words like \"a\", \"the\" etc\n\nmyCorpus = tm_map(myCorpus, stemDocument) # removes the last few letters of similar words such as get, getting, gets\n\nmyCorpus = tm_map(myCorpus, removeNumbers)\n\ndtm = DocumentTermMatrix(myCorpus) #turns the corpus into a document term matrix\n\n\namazonreview_td <- tidy(dtm)\n\namazonreview_sentiments <- amazonreview_td %>%\n  inner_join(get_sentiments(\"bing\"), by = c(term = \"word\"))\n\namazonreview_sentiments\n\n### We can find the most negative documents:\nlibrary(tidyr)\n\namazonreview_sentiments %>%\n  count(document, sentiment, wt = count) %>%\n  ungroup() %>%\n  spread(sentiment, n, fill = 0) %>%\n  mutate(sentiment = positive - negative) %>%\n  arrange(sentiment)\n\n## Or visualize which words contributed to positive and negative sentiment:\nlibrary(ggplot2)\n\namazonreview_sentiments %>%\n  count(sentiment, term, wt = count) %>%\n  ungroup() %>%\n  filter(n >= 2) %>%\n  mutate(n = ifelse(sentiment == \"negative\", -n, n)) %>%\n  mutate(term = reorder(term, n)) %>%\n  ggplot(aes(term, n, fill = sentiment)) +\n  geom_bar(stat = \"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +\n  ylab(\"Contribution to sentiment\")\n\n#### \n\nreviews.sent <- dfm(myCorpus)\nreviews.sent[,1:5]\n\n\n\n\n# loading packages\n#library(ROAuth)\nlibrary(tidyverse)\nlibrary(text2vec)\nlibrary(caret)\nlibrary(glmnet)\nlibrary(ggrepel)\n\n\nconv_fun <- function(x) iconv(x, \"latin1\", \"ASCII\", \"\")\n\nreviews.read <- read.csv(\"reviews/ReviewSentiment.csv\")  %>%\n  # converting some symbols\n  dmap_at('Review', conv_fun) %>%\n  # replacing class values\n  mutate(sentiment = ifelse(label == \"Positive\", 1, 0)) %>% select(Review,sentiment)\n\n\n\n# data splitting on train and test\nset.seed(2322)\nsmp_size <- floor(0.80 * nrow(reviews.read))\n\ntrain_ind <- sample(seq_len(nrow(reviews.read)), size = smp_size)\namazon_train <- reviews.read[train_ind,]\namazon_test <- reviews.read[-train_ind,]\n\n\n\n\n\n##### doc2vec #####\n# define preprocessing function and tokenization function\nprep_fun <- tolower\ntok_fun <- word_tokenizer\n\n\nit_train <- itoken(amazon_train$Review, \n                   preprocessor = prep_fun, \n                   tokenizer = tok_fun,\n                   progressbar = TRUE)\n\nit_test <- itoken(amazon_test$Review,\n                  preprocessor = prep_fun, \n                  tokenizer = tok_fun,\n                  progressbar = TRUE)\n\n\n\n# creating vocabulary and document-term matrix\nvocab <- create_vocabulary(it_train)\nvectorizer <- vocab_vectorizer(vocab)\ndtm_train <- create_dtm(it_train, vectorizer)\ndtm_test <- create_dtm(it_test, vectorizer)\n\n# define tf-idf model\ntfidf <- TfIdf$new()\n\n# fit the model to the train data and transform it with the fitted model\ndtm_train_tfidf <- fit_transform(dtm_train, tfidf)\ndtm_test_tfidf <- fit_transform(dtm_test, tfidf)\n\n\n# train the model\nt1 <- Sys.time()\nglmnet_classifier <- cv.glmnet(x = dtm_train_tfidf, y = amazon_train[['sentiment']], \n                               family = 'binomial', \n                               # L1 penalty\n                               alpha = 1,\n                               # interested in the area under ROC curve\n                               type.measure = \"auc\",\n                               # 5-fold cross-validation\n                               nfolds = 5,\n                               # high value is less accurate, but has faster training\n                               thresh = 1e-3,\n                               # again lower number of iterations for faster training\n                               maxit = 1e3)\nprint(difftime(Sys.time(), t1, units = 'mins'))\n\n\nplot(glmnet_classifier)\nprint(paste(\"max AUC =\", round(max(glmnet_classifier$cvm), 4)))\n\n\npreds <- predict(glmnet_classifier, dtm_test_tfidf, type = 'response')[ ,1]\nglmnet:::auc(as.numeric(amazon_test$sentiment), preds)\n",
    "created" : 1492008018432.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2709517800",
    "id" : "90AA4378",
    "lastKnownWriteTime" : 1492106956,
    "last_content_update" : 1492106956994,
    "path" : "C:/Users/lalit/Dropbox/NEU_Curriculum/SEM5-Spring2017/BigData-Analytics/Final_Project/SentimentAnalysis/Testing.R",
    "project_path" : "Testing.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}